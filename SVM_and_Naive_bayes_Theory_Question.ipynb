{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##1. What is a Support Vector Machine(SVM)?"
      ],
      "metadata": {
        "id": "zdO4ubD8VJhO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans: A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. However, it is mostly used for classification problem\n",
        "\n",
        "####Key Concept in SVM\n",
        "```\n",
        "1. Hyperplane\n",
        "2. Support Vectors\n",
        "3. Margin\n",
        "4. Kernel Trick\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ikS3JK6pVS46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. What is the difference between Hard Margin and Soft Margin SVM?"
      ],
      "metadata": {
        "id": "QoZ_zMlSVuvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- Hard Margin:-\n",
        "\n",
        "\n",
        "```\n",
        "* Hard Margin SVM is used when the data is completely separable (i.e., a straight line can perfectly divide the classes).\n",
        "* It tries to find the best possible hyperplane without allowing any misclassification.\n",
        "*No misclassified Points\n",
        "* Strict decision boundary\n",
        "* Only Works for linearly separable data\n",
        "\n",
        "```\n",
        "\n",
        "####Soft Margin:-\n",
        "\n",
        "```\n",
        "* Soft Margin SVM is used when the data is not perfectly separable and allows some misclassification to improve generalization.\n",
        "* It introduces a slack variable (Œæ) that allows some points to be on the wrong side of the margin.\n",
        "* Allows some misclassification.\n",
        "* Works well with noisy or overlapping data\n",
        "* More flexible and generlizes better\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RjEqUUjzVzaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. What is the mathematical intution behind SVM?"
      ],
      "metadata": {
        "id": "gTBi4PR8Wse-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- SVM (Support Vector Machine) is a powerful classification algorithm that aims to find the best possible decision boundary (hyperplane) between different classes. The key mathematical idea behind SVM is maximizing the margin between the classes while minimizing classification errors.\n",
        "\n"
      ],
      "metadata": {
        "id": "pm_stOjfW3ej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. What is the role of Language Multipliers in SVM?"
      ],
      "metadata": {
        "id": "dLVn8nWFW5Jr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Lagrange Multipliers play a crucial role in solving the optimization problem in Support Vector Machine (SVM). They help in maximizing the margin between two classes while satisfying constraints."
      ],
      "metadata": {
        "id": "nheNfyXWXFTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. What are Support Vector in SVM?"
      ],
      "metadata": {
        "id": "wQfoZHeuXJ8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- Support Vectors are the data points that are closest to the decision boundary (hyperplane) and play a crucial role in defining the position and orientation of the hyperplane in Support Vector Machine (SVM).\n",
        "\n",
        "####These points determine the maximum margin and are the most important elements in training an SVM model."
      ],
      "metadata": {
        "id": "3ccamhKgXhBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. What is a Support Vector Classifier(SVC)?"
      ],
      "metadata": {
        "id": "8QI1fcTjXxen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:-A Support Vector Classifier (SVC) is a classification model based on Support Vector Machines (SVM). It is used to find the optimal hyperplane that separates different classes while maximizing the margin between them."
      ],
      "metadata": {
        "id": "NmZt6U9hX9C8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. What is a Support Vector Regressor(SVR)?"
      ],
      "metadata": {
        "id": "7vn-PtAJX-p2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- Support Vector Regressor (SVR) is a type of Support Vector Machine (SVM) used for regression tasks. Instead of finding a hyperplane that separates data into classes (as in classification), SVR finds a hyperplane that best fits the data while allowing some error (tolerance margin)."
      ],
      "metadata": {
        "id": "GnAw17UuYPOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8. What is the Kernel Trick in SVM?"
      ],
      "metadata": {
        "id": "L8sBWJGrYRjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:-The Kernel Trick is a technique used in Support Vector Machines (SVM) to transform non-linearly separable data into a higher-dimensional space, where it becomes linearly separable.\n",
        "\n",
        "####Since directly mapping data into a higher dimension is computationally expensive, the Kernel Trick computes this transformation implicitly without actually performing the transformation, saving time and resources."
      ],
      "metadata": {
        "id": "vo2igZnXYaKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernal?"
      ],
      "metadata": {
        "id": "Cc7sWeL7YdPi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:-\n",
        "\n",
        "Linear Kernel:-\n",
        "\n",
        "```\n",
        "The simplest kernel, used when data is already linearly separable.\n",
        "It computes the dot product between two feature vectors.\n",
        "\n",
        "```\n",
        "\n",
        "Polynomial Kernel:-\n",
        "\n",
        "```\n",
        "Captures non-linear relationships between features.\n",
        "Can create curved decision boundaries by increasing the polynomial degree.\n",
        "Higher-degree polynomials can lead to overfitting.\n",
        "```\n",
        "\n",
        "RBF Kernal:-\n",
        "\n",
        "\n",
        "```\n",
        "Maps data into an infinite-dimensional space.\n",
        "Creates highly flexible decision boundaries.\n",
        "Works well for complex, non-linear data.\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t7mk1hbnZJ5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10. What is the effect of the C parameter in SVM?\n"
      ],
      "metadata": {
        "id": "nXUIf3HKZkuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- High C (Large Value)\n",
        "\n",
        "```\n",
        "The model prioritizes correctly classifying all training points over having a wide margin.\n",
        "Leads to a smaller margin but fewer misclassifications.\n",
        "Risk: Can lead to overfitting because it tries too hard t\n",
        "```\n",
        "\n",
        "\n",
        "####Low C (Small Value)\n",
        "\n",
        "\n",
        "```\n",
        "The model allows some misclassifications to maintain a larger margin.\n",
        "The decision boundary is smoother and more generalized.\n",
        "Risk: Can lead to underfitting because it may ignore some important patterns in the data.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "PhQBSL5ZZ3xE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##11 What is the role of the Gamma parameter in RBF Kernel SVM?\n"
      ],
      "metadata": {
        "id": "drduJ8S1aIyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:-In Radial Basis Function (RBF) Kernel SVM, the Gamma (Œ≥) parameter controls how far the influence of a single training point reaches. It determines the complexity of the decision boundary and how much each support vector affects the classification.\n",
        "\n"
      ],
      "metadata": {
        "id": "9R2CEq-QaTCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##12. What is the Na√Øve Bayes classifier, and why is it called Naive?\n"
      ],
      "metadata": {
        "id": "KiuhyOg2aajS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:-The Na√Øve Bayes classifier is a probabilistic machine learning algorithm based on Bayes' Theorem. It is used for classification tasks, especially in text classification (spam detection, sentiment analysis) and medical diagnosis.\n",
        "\n",
        "\n",
        "####Why is it Called \"Naive\"?\n",
        "The classifier is called \"Na√Øve\" because it makes a strong assumption:\n",
        "\n",
        "üîπ All features are independent of each other, given the class label.\n",
        "\n",
        "In reality, this assumption is rarely true (features are often correlated), but Na√Øve Bayes still performs well in many cases."
      ],
      "metadata": {
        "id": "giNccfomaqUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##13. What is Bayes Theorem?\n"
      ],
      "metadata": {
        "id": "6O-FEcuda26j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- Bayes' Theorem is a fundamental rule in probability that describes how to update our beliefs based on new evidence. It is widely used in machine learning, statistics, medical diagnosis, spam detection, and AI applications."
      ],
      "metadata": {
        "id": "AKoSGMv1bLWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##14. Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes.\n"
      ],
      "metadata": {
        "id": "MUEyocv_bNed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:-\n",
        "1. Gaussian Na√Øve Bayes (GNB)\n",
        "\n",
        "\n",
        "```\n",
        "Used for: Continuous (Numerical) Data\n",
        "Assumption: Features follow a normal (Gaussian) distribution.\n",
        "\n",
        "Example:- Weather Prediction ‚Üí Temperature (continuous data) is modeled using a Gaussian distribution.\n",
        "Medical Diagnosis ‚Üí Blood pressure or sugar levels (numerical values).\n",
        "```\n",
        "\n",
        "2. Multinomial Na√Øve Bayes (MNB)\n",
        "\n",
        "\n",
        "```\n",
        "Used for: Discrete (Count-Based) Data\n",
        "Assumption: Features represent frequency counts of words or categorical values.\n",
        "\n",
        "Example:\n",
        "Text Classification ‚Üí Spam detection, sentiment analysis, news categorization (word frequency).\n",
        "Document Classification ‚Üí Number of times a word appears in a document.\n",
        "\n",
        "```\n",
        "\n",
        "3. Bernoulli Na√Øve Bayes (BNB)\n",
        "\n",
        "\n",
        "```\n",
        "Used for: Binary (Yes/No or 0/1) Data\n",
        "Assumption: Features are boolean (binary) (presence or absence of a feature).\n",
        "\n",
        "Example:\n",
        "Text Classification ‚Üí Whether a word appears in an email (spam detection).\n",
        "Fraud Detection ‚Üí Whether a transaction has suspicious activity (yes/no).\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jPitHiIKbXgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## 15. When should you use Gaussian Na√Øve Bayes over other variants?\n"
      ],
      "metadata": {
        "id": "tJjZmUGVcLGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- Gaussian Na√Øve Bayes (GNB) is best suited for continuous (numerical) data that follows a normal (Gaussian) distribution. You should use GNB instead of Multinomial or Bernoulli Na√Øve Bayes in the following cases:-\n",
        "\n",
        "```\n",
        " Features Are Continuous (Numerical Data)\n",
        " Data Follows a Normal (Gaussian) Distribution\n",
        " You Have Limited Training Data\n",
        " Features Are Independent\n",
        " When You Need a Fast, Simple Model\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "nv2ULKMOciqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##16. What are the key assumptions made by naive Bayes?"
      ],
      "metadata": {
        "id": "i-GhCWikc7KZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:-The Na√Øve Bayes classifier is based on Bayes' Theorem and makes several assumptions to simplify probability calculations. These assumptions, while often unrealistic, allow the algorithm to be fast, efficient, and effective in many classification tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "6T159aMddGW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##17. What are the advantages and disadvantages of Na√Øve Bayes?"
      ],
      "metadata": {
        "id": "8YgU57R7dMfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:-\n",
        "Advantages:-\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "1. Fast and Efficient\n",
        "2. Works Well the High-Dimensional Data\n",
        "3. Simple to Implement and Interpret\n",
        "4. Can Handle Missing Data Well\n",
        "5. Performs Well on Certain Types of Data\n",
        "\n",
        "```\n",
        "Disadvantages:-\n",
        "\n",
        "\n",
        "```\n",
        "1. Assumes Feature Independence (Unrealistic in Many Cases)\n",
        "2. Poor Performance with Small Data and Low Feature Counts\n",
        "3. Struggles with Continuous Data (Unless Using Gaussian Na√Øve Bayes)\n",
        "4. Not Suitable for Highly Complex Datasets\n",
        "5. Sensitive to Noisy Data\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2ehlex9wda9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##18. Why is Naive Bayes a good choice for text classification?"
      ],
      "metadata": {
        "id": "afzObR_Yd_zl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- Reasons Why Naive Bayes is Good for Text:-\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "1. Works Well with High-Dimenisional Data\n",
        "2. Fast and Scalable\n",
        "3. Simple and Easy to Implement\n",
        "4. Handle Noisy Data Well\n",
        "5. Works Well even with Small Training Data\n",
        "6. Performs Well in Text-Based Problems\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "cTUlJrdzeJ-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##19. Compare SVM and Na√Øve Bayes for classification tasks?\n"
      ],
      "metadata": {
        "id": "5wwaBpAYelg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- SVM:-\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "1. Discriminative model\n",
        "2. Finds an optimal decision boundary (hyperplane)\n",
        "3. No assumptions about feature independence\n",
        "4. Works well with small datasets\n",
        "5. Can struggle if dimensions are very high\n",
        "6. Slower, especially for large datasets\n",
        "7. Harder to interpret\n",
        "8. \tImage classification, bioinformatics, financial analysis\n",
        "```\n",
        "\n",
        "Naive Bayes(NB):-\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "1. Generative model\n",
        "2. Uses Bayes' theorem to calculate probabilities\n",
        "3. Assumes features are independent\n",
        "4. Performs well even with very small data\n",
        "5. Works well with high-dimensional text data\n",
        "6. Very fast, even on large datasets\n",
        "7. Can be sensitive to noisy features\n",
        "8. Spam filtering, sentiment analysis, document classification\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8R2zM3ZYesv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##20. How does Laplace Smoothing help in Na√Øve Bayes?\n"
      ],
      "metadata": {
        "id": "sqHB7-HQfeLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of using raw word counts, we add a small value (usually 1) to all word counts before computing probabilities.\n",
        "\n",
        "Formula (Before Smoothing):\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùë§\n",
        "ùëñ\n",
        "‚à£\n",
        "ùê∂\n",
        ")\n",
        "=\n",
        "count\n",
        "(\n",
        "ùë§\n",
        "ùëñ\n",
        ",\n",
        "ùê∂\n",
        ")\n",
        "‚àë\n",
        "count\n",
        "(\n",
        "ùë§\n",
        ",\n",
        "ùê∂\n",
        ")\n",
        "P(w\n",
        "i\n",
        "‚Äã\n",
        " ‚à£C)=\n",
        "‚àëcount(w,C)\n",
        "count(w\n",
        "i\n",
        "‚Äã\n",
        " ,C)\n",
        "‚Äã\n",
        "\n",
        "Formula (After Laplace Smoothing):\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùë§\n",
        "ùëñ\n",
        "‚à£\n",
        "ùê∂\n",
        ")\n",
        "=\n",
        "count\n",
        "(\n",
        "ùë§\n",
        "ùëñ\n",
        ",\n",
        "ùê∂\n",
        ")\n",
        "+\n",
        "1\n",
        "‚àë\n",
        "count\n",
        "(\n",
        "ùë§\n",
        ",\n",
        "ùê∂\n",
        ")\n",
        "+\n",
        "ùëâ\n",
        "P(w\n",
        "i\n",
        "‚Äã\n",
        " ‚à£C)=\n",
        "‚àëcount(w,C)+V\n",
        "count(w\n",
        "i\n",
        "‚Äã\n",
        " ,C)+1\n",
        "‚Äã\n",
        "\n",
        "where:\n",
        "\n",
        "ùë§\n",
        "ùëñ\n",
        "w\n",
        "i\n",
        "‚Äã\n",
        "  = Word in the vocabulary\n",
        "ùê∂\n",
        "C = Class (Spam or Not Spam)\n",
        "ùëâ\n",
        "V = Total number of unique words in the vocabulary\n",
        "By adding 1 to all word counts, we ensure that no probability becomes zero, making the model more robust."
      ],
      "metadata": {
        "id": "FWsimJzefmTQ"
      }
    }
  ]
}